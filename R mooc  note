R programming lecture 2021-3 ~

##6-1 두 그룹간 평균비교분석 (t-test)
단일표본의 평균검정
가설1 : 변수의 평균은 10인가? (one sample t-test)
t.test(변수, mu=검정하고자 하는 평균값)

가설2 : ~(a, b)에 따른 평균에 차이가 있는가? (양측검정, two sample t-test) 
      타겟변수(y축 값), 범주형 변수(x축 값, 위에서 a, b)
t.test(타겟변수~범주형변수, data= )
boxplot(타겟변수~범주형변수, boxwex = 0.5, col = c("yellow", "coral"))

단측검정: 기각역이 한쪽에만 있는 경우
      alternative=c("greater") 또는 alternative=c("less")
t.test(타겟변수~범주형변수, data=  , alternative=c("less"))

두 집단의 비모수적 비교검정 : wilcox.test(x, y) 타겟변수가 서열척도(통증, 만족도 등)일때 사용
wilcox.test(타겟변수~범주형변수)


##6-2 짝을 이룬 그룹간 평균비교
paried t-test : t.test(before, after, mu=0, paired=T)
특정 처리(treatment)의 효과를 비교분석 할때 사용, 동일 표본의 before & after 측정
예) 특정 약의 복용 전후 혈압 비교


##6-3 분산분석(ANOVA)
전체분산을 분할하여 어떤 요인의 영향이 유의한지 검정
Factor가 한개일때(one-way ANOVA)
aov(타겟변수~factor)

사후검정 : ANOVA에서 어떤 factor의 유의성이 검정되면 그 다음에 하는 검정
TukeyHSD()
plot(TurkeyHSD())


##7-1 상관분석

cor(변수1, 변수2)

pearson의 상관계수 : cor(옵션없는 경우)
kendall, spearman의 상관계수 : cor(변수1, 변수2, method=c("spearman"))

예) pairs(car1[변수], cex=1, col=as.integer(car1$cyl), pch=19)
                              색을 특정 변수에 넣는 것
상관계수 r 은 0-1 사이의 값. 1에 가까울 수록 강한 상관성

통계치와 그래프 - monkey 데이터 + king kong 한마리 (데이터가 하나 잘 못 들어간 것이 존재할 때)
->상관관계에 대한 해석을 완전히 바꿘 놓을 수 있음


##7-2 선형회귀모형
단순회귀모형 : lm(y변수(=종속변수)~x변수(=독립변수), data= )    linear model -> lm
plot(x축변소, y축변수)
abline : ad line(선을 추가하는 함수)
예) plot(변수, 변수, col=as.integer(car$cyl), pch=19)
    abline(lm(변수~변수), col="red", lwd=2, lty=1)    #best fit linear line

최소자승법 : 예측값과 관측치간의 오차를 최소화 시키는 회귀계수를 추정
회귀식에 의해 설명되는 부분(SSR)과 설명되지 않는 부분(SSE)


##7-3 회귀모형 진단과 평가  -> 컴에서 들어야 함



##8-1 다중회귀분석
데이터 마이닝, 기계학습 표!
타겟변수 값이 주어지는 경우(변수간의 관계) -> 예측, 분류
타겟변수 값이 없는 경우(개체간의 관계) -> 군집, 연관규칙

예측(prediction) -> 회귀분석, 선형모형, 비선형모형
분류(classification) -> 의사결정나무, 서포트벡터머신, 판별분석, 로지스틱회귀모형

autompg 데이터 활용한 연습 -> mpg(연비)를 종속변수(y)값으로, 나머지를 독립변수(x)

lm(y변수~x1+x2+x3, data= )

예) r1 <- lm(mpg ~ disp+hp+wt+accler, data=car)

단계별 방법(stepwise method)
step(모형, directions="both")

다중공선성 -> 독립변수들 사이에 상관관계가 너무 높아서 회귀계수 해석 불가능

분산팽창계수(VIF) -> 다중공선서의 척도
vif(다중회귀모형)

예)vif(lm(mpg ~ disp+hp+wt+accler, data=car)


##8-2 데이터마이닝과 분류

분류(classification)
오분류율 = 오분류 객체수/전체 객체수
교차검증(cross-validation) : n=100 이면 5등분으로 나누어 4등분은 학습데이터로 예측모형을 구성하고, 나머지 5등분 째 데이터로 검증


##8-3 학습데이터와 검증데이터

iris <- read.csv(file="iris.csv")   #150개 데이터
set.seed(100)  #set.seed는 난수 생성 시 처음 시작값을 주어 둥일한 훈련포본 사용, 지정하지 않으면 매번 다른 훈련표본 생성
               #train/test를 2:1로 랜덤 분할(100/50, n=150)
N=nrow(iris)
tr.idx=sample(1:N, size=N*2/3, replace=FALSE)    #tr.idx는 100개의 무작위로 선정된 100개의 데이터 아이디
tr.idx

iris.train <- iris[tr.idx, -5]   #5번째 열의 종속변수를 제외한 100개의 데이터
iris.test <- iris[-tr.idx, -5]   #5번째 열의 종속변수를 제외한 50개의 데이터

trainLables <- iris[tr.idx, 5]
testLables <- iris[-tr.idx, 5]



##9-1 k-인접기법 

최적 k : 교차검증으로 정확도가 높은 k를 선정
장점 : 계산이 단순하고 효율적
단점 : 범주형변수에 대해서는 거리를 계산할 수 없음

kNN을 수행하기 위한 추가 패키지 설치 - class, caret, scales
class : kNN 수행을 위한 패키지
caret : Confusion matrix를 생성하기 위한 패키지
scales : 최적 k 등 그래프를 위한 패키지

iris 데이터 활용
iris <- read.csv(file="iris.csv", stringsAsFactors = TRUE)
set.seed(1000)
n <- nrow(iris)
tr.idx <- sample.int(n, size = round(2/3*n))

iris.train <- iris[tr.idx, -5]   #독립변수 4개를 포함한 100개의 데이터
iris.test <- iris[-tr.idx, -5]   #독립변수 4개를 포함한 50개의 데이터

trainLables <- iris[tr.idx, 5]    #학습데이터의 타겟변수
testLables <- iris[-tr.idx, 5]    #검증데이터의 타겟변수

kNN함수 : kNN(train=학습데이터, test=검증데이터, cl=타겟변수, k= )
예) md1 <- knn(train=iris.train, test=iris.test, cl=trainLabels, k=5)
    md1   #test데이터(50개)들을 예측한 결과가 저장되어 있음

confusionMatrix(md1, testLabels)  #md1=예측값, testLabels=타겟변수의 실제값

md1, testLabel의 유형이 char(문자), factor(범주) 다를때는 오류가 생김, 그래서 처음에 불러올때 stringsAsFactor 해줌

최적 k의 탐색 : 1 to nrow(train_data)/2 (여기서는 1 to 50까지)
accuracy_k <- Null
nnum <- nrow(iris.train)/2
for(kk in c(1:nnum))
{
      set.seed(1234)
      knn_k <- knn(train=iris.train, test=iris.test, cl+trainLabels, k=kk)
      accuracy_k <- c(accuracy_k, sum(knn_k==testLabels)/length(testLabels))
}      

test_k <- data.frame(k=c(1:nnum), accuracy=accuracy_k[c(1:nnum)])
plot(formula=accuracy~k, data=test_k, type="o", ylim=c(0.5, 1), pch=20, col=3, main="validation-optimal k")
with(test_k, test(accuracy~k, labels=k, pos=1, cex=0.7)

min(test_k[test_k$accuracy %in% max(accuracy_k), "k"])

k=7에서 정확도(.98)가 가장 높음

최종 kNN모형(k=7)
md2 <- knn(train=iris.train, test=iris.test, cl=trainLabels, k=7)
confusionMatrix(md2, testLabels)

kNN모형(k=7) 결과 - 그래프
plot(formula = Petal.Lenth ~ Petal.width,
      data=iris.train, 
      col=alpha(c("purple", "blue", "green"), 0.7)[trainLabels],
      main="knn(k=7)")
point(formula = Petal.Lenth ~ Petal.width,
      data=iris.test,
      pch=17,
      cex=1.2,
      col=alpha(c("purple", "blue", "green"), 0.7)[md2])
legend("bottonright",
      c(paste("train", levels(trainLables)), paste("test, levels(testLabels))),
      pch=c(rep(1,3), rep(17,3)),
      col=(c(rep(alpha(c("purple", "blue", "green"), 0.7),2)),
      cex=0.9)
      

##9-2 판별분석1  (이론 내용이 어렵다!!)

-객체를 몇 개의 범주로 분류
-범주들을 가장 잘 구분하는 변수 파악 및 범주간 차이를 가장 잘 표현하느 함수 도출
-의사결정이론 : 선형판별분석, 이차판별분석

-선형판별분석 : 오분류 총 확률 = P{범주 1로 오분류} + P{범주 2로 오분류}

iris <- read.csv("iris.csv", stringsAsFactors = TRUE)
attach(iris)

set.seed(1000)
n <- nrow(iris)
tr.idx <- sample.int(n, size=round(2/3*n))

iris.train <- iris[tr.idx, -5]
iris.train <- iris[-tr.idx, -5]

MASS 패키지 설치
LDA함수 : ida(종속변수 ~ 독립변수, data=학습 데이터 이름, prior=사전 확률)

iris.lda <- ida(Species ~ ., data=train, prior=c(1/3, 1/3, 1/3))
iris.lda

참고) Species ~. 는 Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width 와 같은 의미(독립변수 전체를 의미)
      사전확률(prior probability) : 원인 A가 발생할 확률인 P(A)와 같이 결과가 나타나기 전에 결정되어 있는 확률
      
testpred <- predict(iris.lda, test)

정확도 산정 : 오분류율(검증데이터)
confusionMatrix(testpred$class, testLabels)


##9-3 판별분석2 

선형판별분석 vs. 이차판별분석

이차판별분석(QDA) : 모집단 등분산 검정 -> 분산-공분산 행렬이 범주별로 다른 경우, 이차판별분석을 실시
=> Box's M-test


LDA : 두 범주를 분류하는 선형판별함수, 범주간 동일한 분산-공분산행렬을 가정
QDA : 판별함수가 변수들에 대한 이차함수로 표현, 공분산 행렬이 범주별로 다른 경우

이차판별분석(QDA) 
등분산검정을 위한 패키지 : biotools

library(biotools)
boxM(iris[1:4], iris$Species)

QDA함수 : qda(종속변수 ~ 독립변수, data=학습데이터 이름, prior=사전확률)

iris.qda <- qda(Species ~ ., data=train, prior=c(1/3, 1/3, 1/3))
iris.qda

testpredq <- predict(iris.qda, test)

confusionMatrix(testpredq$class, testLabels)


##10-1 서포트벡터머신

분류모델 중 하나
장점 : 정확도가 상대적으로 좋음, 다양한 데이터(연속형, 범주형) 사용가능
단점 : 해석하기 어려움, 데이터가 많을 때 속도가 걸림

선형 SVM : H와 평행인 두 하이퍼플레인
H1과 H2간의 거리를 최대로 하는 분리 하이퍼플레인을 찾자!

비선형 SVM : 대부분의 패턴은 선형적으로 분리 불가능
비선형 패턴의 입력공간을 선형패턴의 'feature space'로 변환
kernel method로 비선형 경계면 도출

library(e1071)   : e1071은 svm 함수 사용을 위한 패키지
library(caret)   : caret은 confusionMatrix 사용을 위한 패키지

데이터 불러오기, stringsAsFactor=TRUE

svm(y변수 ~ x변수, data= )

m1 <- svm(Species ~., data=iris)
summary(m1)

svm모델에 적용한 예측범주와 실제범주 비교(전체 데이터를 사용한 결과)

x <- iris[, -5]
pred <- predict(m1, x)

y <- iris[, -1]
confusionMatrix(pred, y)

시각화
plot(ma, iris, Petal.Width~Petal.Length, slice=list(Sepal.Width=3, 

Petal.Width~Petal.Length가 가장 중요한 변수라는 것을 이미 알고 있는 상황)
 

##10-2 서포트벡터머신2

kernel 함수 

커널(kernel)이란 : 저차원을 고차원으로 변환시켜 주는 함수, 변환을 통해 X에 대한 새로운 특징을 추출할 수 있도록 함

set.seed(1000)
N=norw(iris)
tr.idx=sample(1:N, size=n=*2/3)

iris.train <- iris[tr.idx,]
iris.train <- iris[-tr.idx,]

m1 <- svm(Species~., data=train)
summary(m1)

m2 <- svm(Species~., data=train.kernel="Polynomianl")
summary(m2)

m3 <- svm(Species~., data=train.kernel="sigmoid")
summary(m3)

각 kernel 함수에 따른 결과 비교, 각 함수마다 오분류 되는 것이 다름
pred11 <- predict(m1, test)   #m1 자리에 m2, m3 차례로 입력하여 결과값 확인
confusionMatrix(pred11, test$Species)


m4 <- svm(formula = Species~., data=train.kernel="liner")
summary(m4)



##10-3 서포트벡터머신3

library(e1071)
library(caret)

cancer <- read.csv("cancer.csv", stringsAsFactor = TRUE)
head(cancer, 10)

cancer <- cancer[. names(cancer) != "X1"]   #첫번째 컬럼인 ID넘버는 필요없는 feature이므로 제거
attach(cancer)

N=nrow(cancer)
set.seed(998)

tr.idx=sample(1:n, size=N*2/3, replace=FALSE)  #데이터 분할, 학습데이터 2/3, 검증데이터 1/3
train <- cancer[tr.idx,]
test <- cancer[-tr.idx,]

kernel 함수에 따른 서포트벡터머신
m1 <- svm(Y ~., data=train)   #radial
summary(m1)

m2 <- svm(Y ~., data=train, kernel="polymomial")   #polymomial
summary(m2)

m3 <- svm(Y ~., data=train, kernel="sigmoid")   #sigmoid
summary(m3)

m4 <- svm(Y ~., data=train, kernel="linear")   #linear
summary(m4)

정확도 측정 #m1~m4까지 각 측정
pred11 <- predict(m1, test)
confusionMatrix(pred11, test$Y)


##11-1 의사결정나무1

의사결정나무 : 의사결정 규칙을 나무형태로 분류해 나가는 분석 기법, 기계학습 중 하나
               분석과정이 직관적이고 이해하기 쉬움, 연속형/범주형 변수 모두 사용할 수 있음, 분지 규칙은 불순도를 최소화 시킴
             
tree형성(Growing tree)
tree가지치기(Pruning tree)
최적 tree로 분류(Classification)

의사결정나무 실행 피키지 : tree(그외 rpart, party 패키지)
library(tree)
library(caret)

iris <- read.csv("iris.csv", stringsAsFactors = TRUE)
set.seed(1000)
N <- nrow(iris)
tr.idx <- sample91:n, size=N*2/3, replace=FALSE)

train <- iris[tr.idx,]
test <- iris[-tr.idx,]

treemod <- tree(Species ~., data=train)
treemod
plot(treemod)   #의사결정나무 분지를 그림으로 표현
text(treemod, cex=1.5)   #cex : 폰트사이즈

tree의 결과(*는 터미널노드) : 마디 6에서는 더 이상 분지할 필요 없음

최적 tree모형을 위한 가지치기(pruning) : cv.tree(tree모형결과, FUN=)
결과에서 교차검증오차(deviance)의 값이 최소가 되는 노드 수 선택

cv.trees <- cv.tree(treemod, FUN=prune.misclass)
cv.trees
plot(cv.trees)

par(mfrow=c(1,2))
plot(cv.trees$size, cv.trees$dev, type="b")
plot(cv.trees$k, cv.trees$dev, type="b")

pruning(가지치기) : cv.tree함수를 이용하여 최적 터미널노드를 탐색

prune.trees <- prune.misclass(treemod, bext=3)  #prune.trees는 최종모형의 이름(최종터미널노드=3)
plot(prune.trees)
text(prune.trees, pretty=0, cex=1.5)

의사결정나무결과 정확도 : test data에 대한 정확도
treepred <- predict(prune.trees, test, type='class')
confusionMatrix(treepred, test$Species)



##11-2 의사결정나무2

의사결정나무 실행 패키지 : rpart패키지 (tree패키지 외 사용)

library(rpart)
library(party)
library(caret)

의사결정나무 함수 : rpart(종속변수 ~ x1+x2+x3+x4, data= )
cl1 <- rpart(Species ~., data=train)
plot(cl1)
text(cl1, cex=1)

rpart패지에서의 최적 트리모델(cp(compexity parameter)비교)
printcp에서 xerror(cross validation error)의 값이 최소가 되는 트리를 선택
printcp(cl1)
plotcp(cl1)

rpart를 사용한 최종 tree모형
pcl1 <- prune(cl1, cp=li1$cptable[which.min(cl1$cptalbe[, "xerror"]), "CP"])
plot(pcl1)
text(pcl1)

pred2 <- predict(pcl1, test, type='class')
confusionMatrix(pred2, test$Species)

의사결정나무 실행 피키지 : party 패키지
ctree(종속변수 ~ x1+x2+x3+x4, data= )
partymod <- ctree(Species ~., data=train)
plot(partymod)
partymod

partypred <- predict(partymod, test)
confusionMatrix(partypred, test$Species)


##11-3 랜덤포레스트

랜덤포레스트(random forest) : 의사결정나무의 단점(과적합)을 개선한 알고리즘
                              주어진 데이터의 리샘플링을 통해 생성한 다수의 의사결정나무 모델을 결합하여 정확도를 높이는 방법인 ensemble 기법
                             
training data로부터 표본의 크기가 n인 bootstrap sample을 추출 -> tree모형 구성(tree1, treee2, ..., treek) -> 각 모델 tree들의 앙상블 결과로 분류

bagging(bootstrap aggregating) : 전체 데이터에서 샘플을 여러 번 추츨(bootstraㅔ)하고 각 모델을 aggregation하여 최종모델을 도출하는 방법
                              training data 에서 random sampling
                              
랜덤포레스트 패키지 : randomForest

library(randomForest)
metry : 독립변수의 수(numver of variables randomly sampled, default = sqrt(k)
ntree : tree의 수 (number of decision trees to be grown)
sampsize : 샘플사이즈 (sample size to be drawn from the input data for growing decision tree)
importance : 변수중요도

iris <- read.csv("iris.csv")
attach(iris)

set.seed(1000)
N <- nrow(iris)
tr.idx <- sample(1:N, size=N*2/3, replace=FALXE)
train <- iris[tr.idx,]
test <- iris[-tr.idx,]

randomForest(종속변수 ~ x1+x2+x3+x4, data= )
rf_out1 <- randomForest(Species ~ ., data=train, impoortance=T)
rf_out1

rf_out2 <- randomForest(Species ~ ., data=train, impoortance=T, mtry=4)
rf_out2

변수의 중요도 : random forest결과로부터 중요변수 확인
round(importance(rf_out2), 2)
randomForest::importance(rf_out2)
varImpPlot(rf_out2)

랜덤포레스트 결과 정확도 : test data에 대한 정확도
rfpred <- predict(rf_out2, test)
confusionMatrix(rfpred, test$Species)


##12-1 군집분석과 유사성척도

비지도학습(unsupervised learning)이며 속성변수들의 특징으로 그룹화하는 기법
계층형 군집분석, k-means

군집분석(cluster analysis)란, 유사한 속성을 가진 객체들을 군집으로 나누어 묶어주는 데이터 마이닝 기법

계층적 방법과 비계층적 방법
계층적 군집 : hierarchical clustering, 사전에 군집 수 k를 정하지 않고 단계적으로 군집 트리를 제공
비계층적 군집 : non-hierarchical ~, 사전에 군집 수 k를 정한 후 각 객체를 k개 중 하나의 군집에 배정

객체 간의 유사성 정도를 정량적으로 나타내는 척도
거리(distance)척도 : 거리가 가까울수록 유사성이 큼
상관계수척도 : 객체간 상관계수가 클스록 두 객체의 유사성이 커짐

객체 i의 p차원 공간에서의 좌표는 열벡터로 표현
p개의 속성을 가진 객체 i에 대하여, j번째 속성은 Xij로 표현

유클리디안 거리

거리 계산 함수 : dist(데이터, method= , )
m1 <- matrix (
      c(150, 50, 130, 55, 80, 80, 100, 85, 95, 91),
      nrow = 5, 
      ncol = 2,
      byrow = TRUE)

m1
is.data.frame(m1)
m1 <- as.data.frame(m1)

D1 <- dist(m1)
D1

민코프스키 거리 : 유클리디안 거리의 일반화된 방법

마할라노비스 거리 : 변수간의 상관관계가 존재할 때 사용(공분산 척도로 유용)

민코프스키 거리
dist(data(or matrix), method="minkowski", p=3)

D2 <- dist(m1, method="minkowski", p=3)
D2

상관계수측정(cor)
m2 <- matrix (
      c(150, 50, 130, 55, 80, 80, 100, 85, 95, 91),
      nrow = 5, 
      ncol = 2,
      byrow = TRUE)
m2

cor(m2[1,], m2[2,])
cor(m2[1,], m2[3,])



##12-2 계층적 군집분석

사전에 군집 수 k를 정하지 않고 단계적으로 군집을 형성
단일연결법, 완전연결법, 평균연결법, 중심연결법

단일연결법 : 군집 i와 군집 j의 유사성 척도로 두 군집의 모든 객체 쌍의 거리 중 가장 가까운 거리를 사용
완전연결법 : 두 군집의 객체 쌍의 거리 중 가장 먼 거리를 사용
평균연결법 : 두 군집의 모든 객체 쌍의 평균 거리를 사용
중심연결법 : 두 군집의 중심좌표

단일연결법 군집화 과정 : 유클리디안 거리 사용
덴드로그램 : 군집 그룹과 유사성 수준을 표시하는 트리 다이어그램

wages1833.csv 데이터를 가지고 분석

wages1833 <- read.csv(file="wages1833.csv", stringsAsFactors = TRUE)
head(wages1833, n=10)

dat1 <- wages1833[, -1]   #ID변수 제외
dat1 <- na.omit(dat1)   #결측치 데이터 삭제
str(dat1)

dist_data <- dist(dat1)

완전연결법
hc_a <- hclust(dist_data, method = "complete")   #method: single(단일), complete(완전), average(평균), centroid(중심)
plot(hc_a, hang = -1, cex=0.7, main = "complete")

평균연결법
hc_c <- hclist(dist_Data, method = "average")
plot(hc_c, hang=-1, cex=0.7, main = "average")
           #hang: 라벨을 일정한 위치에 고정

워드 연결방법(Ward's method)
hc_c <- hclust(dist_data, method = "ward.D2")
plot(hc_c, hang=-1, cex=0.7, main = "Ward's method")



##12-3 비계층적 군집분석

사전에 군집 수 k를 정한 후 객체를 k개 중 하나의 군집에 배정
k-means알고리즘, k-medoids알고리즘(PAM, CLARA), DBSCAN알고리즘(밀도기반)

k-means 군집분석 : 가장 널리 사용. 중심좌표를 고려하여 각 가체를 가장 가까운 군집에 배정을 반복
초기객체선정 -> 객체군집배정 -> 군집중심좌표 산출 -> 수렵조건 점검

wages1833 <- read.csv(file="wages1833.csv", stringsAsFactors = TRUE)
head(wages1833)

dat1 <- wages1833[, -1]   #ID변수 제외
dat1 <- na.omit(dat1)   #결측치 데이터 삭제
head(dat1, n=5)

library(factoextra)
library(ggplot2)

#최적값은 "silhouette", "gap_stat", "wss(그룹내제곱합)"으로 산출
fivz_nbclust(dat1, kmeans, method = "wss")
fivz_nbclust(dat1, kmeans, method = "gap_stat")
#그래프라 완만해지는 지점을 k의 값으로 추정

#k-means (k=3)
set.seed(123)
km <- kmeans(dat1, 3, nstart = 25)  #random set의 수 (nstart)
km

#kmeans 결과 시각화, convex모양으로 구역 표시, repel을 통해 관측치 표기
fivz_cluster(km, data = dat1,
            ellipse.type = "convex",
            repel = TRUE)   
         
k-medoids 군집분석 : 중앙값을 각 군집의 대표 객체로 사용, 객체와 속하는 군집의 대표 객체와의 거리 총합을 최소로 하는 방법
PAM 알고리즘 : 모든 객체에 대하여 대표 객체가 변했을 때 발생하는 거리 총합의 변화를 계산
CLARA 알고리즘 : 적절한 수의 객체를 샘플링 한 후, PAM 알고리즘을 적용하여 대표 객체 선정

PAM 알고리즘
k=3

library("cluster")
pam_out <- pam(dat1, 3)
pam_out

table(pam_out$clustering)
fivz_cluster(pam_out, data = dat1,
            ellipse.type = "convex",
            repel = TRUE)   

DBSCAN 알고리즘 : 특정 공간 내에 데이터의 밀도 차이를 기반으로 군집화, 자동으로 군집수를 찾음, 기하학적 분포도를 가진 데이터 군집화 수행

library(fpc)
db <- dbscan(dat1, eps=70, Minpts=3)   #Min poubts : 군집을 이루기 위한 최소 원소 개수, Eps: 군지을 이루기 위한 최소 반경
db
db$cluster
fivz_cluster(db, data = dat1,
            ellipse.type = "convex",
            repel = TRUE)   

실루엣계수 : 군집의 성능평가척도
library(cluster)
sil_pam <- silhouette(pan_out$clustering, dist(Dat1))
mean(sil_pam)
sil_dv <- silhouette(db$cluster, dist(dat1))
mean(sil_db)


##13-1 연관규칙1

연관규칙(association rule) : 대용량 거래(트랜잭션) 데이터베이스에서 빈번하게 발생하는 패턴을 발견
                              거래 간의 상호 관련성을 분석
시장바구니
트랜잭션 : 고객이 거래한 정보를 하나의 거래
시장바구니 분석

지지도, 신뢰도, 향상도

향상도 : A가 거래된 경우, 그 거래가 B를 포함하는 경우와 B가 임의로 거래되는 경우의 비율

library(arules) : 연관규칙분석 수행을 위한 라이브러리

연습데이터 : dvdtrans.csv
split를 통해 id별로 item들을 as함수를 통해 transaction data로 변환

dvd1 <- read.csv("dvdtrans.csv")
dvd1

dvd.list <- split(dvd1$Item, dvd1$ID)
dvd.list

dvd.trans <- as(dvd.list, "transactions")
dvd.trans

inspect(dvd.trans)

연관규칙 함수 : apriori(transaction, parameter=list(support=##, confidence=##)

dvd_rule <- apriori(dvd.trans,
                    parameter = list(support=0.2, 
                                    confidence = 0.2,
                                     minlen = 2))
dvd_rule

summary(dvd_rule)
inspect(dvd_rule)


itemFrequencyPlot(dvd.trans, support = 0.2, main = " item for support>=0.2", col="green")


##13-2 연관규칙2

Groceries data(arules 패키지에 탑재되어 잇는 데이터)

library(arules)
data("Groceries")
summary(Groceries)

itemFrequencyPlot(Groceries, support=0.05, main = "items for support>=5%", col="green")
itemFrequencyPlot(Groceries, topN=20, main = "support top 20 items", col="coral")

#연관규칙 분석, support, confidence와 length는 minimum값으로 너무 높게 잡으면 연관규칙 도출이 어려움
Grocery_rule <- apriori(data=Groceries, 
                        parameter = list(support=0.05,
                                          confidence = 0.02,
                                          minlen = 2)
Grocery_rule

summary(Grocery_rule)
inspect(Grocery_rule)

#연관규칙-향상도(lift)순서로 정렬
inspect(sort(Grocery_rule, by="lift")

sort() 함수를 통해 분석가가 보고자 하는 기준으로 정렬
subset() 함수를 통해 원하는 item이 포함된 연관규칙만 추출
%in%, %pin% 을 이용해 다양한 조건의 규칙 도출

rule_interest3 <- subset(Grocery_rule, items %in% c("yogurt"))
inspect(rule_interest3)

rule_interest5 <- subset(Grocery_rule, items %in% c("other") & confidence>0.25)
inspect(rule_interest5)

#연관규칙 결과를 데이터 프레임으로 저장
Grocery_rule_df <- as(Grocery_rule, "data.frame")
Grocery_rule_df

#연관규칙결과 저장
write(Grocery_rule, file="Grocery_rule.csv", 
      set= ",",
      quote=TRUE,
      row.names=FALSE)
      

















